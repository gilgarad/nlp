{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gilgarad/nlp_nlu/blob/master/jupyter_colab/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rpvumrPq2VYm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import merge\n",
        "from keras.layers.core import *\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import *\n",
        "\n",
        "from attention_utils import get_activations, get_data_recurrent\n",
        "\n",
        "INPUT_DIM = 2\n",
        "TIME_STEPS = 20\n",
        "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
        "SINGLE_ATTENTION_VECTOR = False\n",
        "APPLY_ATTENTION_BEFORE_LSTM = False\n",
        "\n",
        "\n",
        "def attention_3d_block(inputs):\n",
        "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "    input_dim = int(inputs.shape[2])\n",
        "    a = Permute((2, 1))(inputs)\n",
        "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
        "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
        "    if SINGLE_ATTENTION_VECTOR:\n",
        "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
        "        a = RepeatVector(input_dim)(a)\n",
        "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
        "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
        "    return output_attention_mul\n",
        "\n",
        "\n",
        "def model_attention_applied_after_lstm():\n",
        "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
        "    lstm_units = 32\n",
        "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
        "    attention_mul = attention_3d_block(lstm_out)\n",
        "    attention_mul = Flatten()(attention_mul)\n",
        "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
        "    model = Model(input=[inputs], output=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_attention_applied_before_lstm():\n",
        "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
        "    attention_mul = attention_3d_block(inputs)\n",
        "    lstm_units = 32\n",
        "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
        "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
        "    model = Model(input=[inputs], output=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    N = 300000\n",
        "    # N = 300 -> too few = no training\n",
        "    inputs_1, outputs = get_data_recurrent(N, TIME_STEPS, INPUT_DIM)\n",
        "\n",
        "    if APPLY_ATTENTION_BEFORE_LSTM:\n",
        "        m = model_attention_applied_before_lstm()\n",
        "    else:\n",
        "        m = model_attention_applied_after_lstm()\n",
        "\n",
        "    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    print(m.summary())\n",
        "\n",
        "    m.fit([inputs_1], outputs, epochs=1, batch_size=64, validation_split=0.1)\n",
        "\n",
        "    attention_vectors = []\n",
        "    for i in range(300):\n",
        "        testing_inputs_1, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
        "        attention_vector = np.mean(get_activations(m,\n",
        "                                                   testing_inputs_1,\n",
        "                                                   print_shape_only=True,\n",
        "                                                   layer_name='attention_vec')[0], axis=2).squeeze()\n",
        "        print('attention =', attention_vector)\n",
        "        assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
        "        attention_vectors.append(attention_vector)\n",
        "\n",
        "    attention_vector_final = np.mean(np.array(attention_vectors), axis=0)\n",
        "    # plot part.\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "\n",
        "    pd.DataFrame(attention_vector_final, columns=['attention (%)']).plot(kind='bar',\n",
        "                                                                         title='Attention Mechanism as '\n",
        "                                                                               'a function of input'\n",
        "                                                                               ' dimensions.')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dyA5a0Wu2mG4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}