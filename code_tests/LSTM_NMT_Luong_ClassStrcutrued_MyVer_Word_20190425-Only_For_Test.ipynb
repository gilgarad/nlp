{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Data download: https://github.com/jungyeul/korean-parallel-corpora\n",
    "# * This implementation is originally from: https://keras.io/examples/lstm_seq2seq/\n",
    "# * And re-implemented 1. data pre-process 2. neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Flatten, Activation\n",
    "from keras.constraints import MinMaxNorm\n",
    "from keras.initializers import RandomUniform\n",
    "from keras import losses\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from os.path import join\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data txt file on disk.\n",
    "# data_path = '/content/gdrive/My Drive/ai_data/kor-eng/kor.txt'\n",
    "os_type = 'linux'\n",
    "\n",
    "if os_type == 'windows':\n",
    "    root_path = 'd:/IGS_Projects_data'\n",
    "else: # os_type == linux\n",
    "    root_path = '/data1'\n",
    "\n",
    "input_data_path = join(root_path, 'translation_data/korean-english-park.train/korean-english-park.train.en')\n",
    "output_data_path = join(root_path, 'translation_data/korean-english-park.train/korean-english-park.train.ko')\n",
    "\n",
    "val_input_data_path = join(root_path, 'translation_data/korean-english-park.dev/korean-english-park.dev.en')\n",
    "val_output_data_path = join(root_path, 'translation_data/korean-english-park.dev/korean-english-park.dev.ko')\n",
    "\n",
    "test_input_data_path = join(root_path, 'translation_data/korean-english-park.test/korean-english-park.test.en')\n",
    "test_output_data_path = join(root_path, 'translation_data/korean-english-park.test/korean-english-park.test.ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here is the not one hot encoded output\n",
    "class Dataset:\n",
    "    def __init__(self, max_seq_length=1000):\n",
    "        \n",
    "        # list of data path of train, validation, test\n",
    "        # For translation, each has input_path, output_path\n",
    "        self.input_tokens = list()\n",
    "        self.target_tokens = list()\n",
    "        self.input_token_index = dict()\n",
    "        self.target_token_index = dict()\n",
    "        self.num_encoder_tokens = 0\n",
    "        self.num_decoder_tokens = 0\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        self.okt = Okt()\n",
    "        \n",
    "        # Temp: to check the distribution of the words and their counted numbers\n",
    "        self._input_tokens = dict()\n",
    "        self._target_tokens = dict()\n",
    "        self.dictionary_freq = 0\n",
    "    \n",
    "    def _read_data_seq2seq(self, data_path_list, token_type='char', kor_tokenizer='custom', \n",
    "                           kor_tokenizer_max_word_char=2, num_samples=10000):\n",
    "        \n",
    "        # temp\n",
    "        onehot_style = False\n",
    "        embedding_version = True\n",
    "        \n",
    "        \n",
    "        if len(data_path_list) > 2:\n",
    "            print('too many data path:', len(data_path_list))\n",
    "            return\n",
    "        \n",
    "        input_texts = []\n",
    "        target_texts = []\n",
    "        \n",
    "        data_docs = list()\n",
    "        \n",
    "        for idx, data_path in enumerate(data_path_list):\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                new_data_lines = f.read().split('\\n')\n",
    "                data_docs.append(new_data_lines)\n",
    "#                 print(len(new_data_lines))\n",
    "            \n",
    "        train_val_test_index = list()\n",
    "                \n",
    "        # 1. Tokenize & Count & Store all tokens even duplicates\n",
    "        for idx, doc_texts in enumerate(data_docs):\n",
    "            \n",
    "            # 1-1. Tokenize\n",
    "            # We use \"tab\" as the \"start sequence\" character\n",
    "            # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "            if idx % 2 == 0: # input text\n",
    "                data_texts = input_texts\n",
    "                data_tokens = self.input_tokens\n",
    "                _data_tokens = self._input_tokens\n",
    "                # Tokenize for english input, char and all tokenizers but custom will be accepted\n",
    "                _doc_texts = [self.tokenizer(sentence=sentence, token_type=token_type, kor_tokenizer=kor_tokenizer, \n",
    "                                            kor_tokenizer_max_word_char=-1)\n",
    "                             for sentence in doc_texts]\n",
    "            else: # target text; put \\t and \\n as for start and end marking\n",
    "                data_texts = target_texts\n",
    "                data_tokens = self.target_tokens\n",
    "                _data_tokens = self._target_tokens\n",
    "                _doc_texts = [self.tokenizer(sentence=sentence, token_type=token_type, kor_tokenizer=kor_tokenizer, \n",
    "                                            kor_tokenizer_max_word_char=kor_tokenizer_max_word_char)\n",
    "                             for sentence in doc_texts]\n",
    "                _doc_texts = [np.insert(tokenized_sentence, [0, len(tokenized_sentence)], ['\\t', '\\n']) \n",
    "                              for tokenized_sentence in _doc_texts if len(tokenized_sentence) > 0]\n",
    "    \n",
    "            \n",
    "            data_texts.extend(_doc_texts)\n",
    "            \n",
    "            # 1-2. Count & Store all tokens even duplicates\n",
    "            if self.num_encoder_tokens != -1:\n",
    "#             if self.num_encoder_tokens == 0:\n",
    "                data_tokens.append('<unk>')\n",
    "                for tokenized_sentence in data_texts:\n",
    "                    for token in tokenized_sentence:\n",
    "                        if token not in data_tokens:\n",
    "                            data_tokens.append(token)\n",
    "                        if token not in _data_tokens:\n",
    "                            _data_tokens[token] = 0\n",
    "                        _data_tokens[token] += 1\n",
    "                \n",
    "                for key, count in dropwhile(lambda key_count: key_count[1] > self.dictionary_freq, Counter(_data_tokens).most_common()):\n",
    "                    del _data_tokens[key]\n",
    "            train_val_test_index.append(len(data_texts))\n",
    "        train_val_test_index = train_val_test_index[::2]\n",
    "                        \n",
    "        # 2. Unique tokens only\n",
    "        self.input_tokens = sorted(list(set(self.input_tokens)))\n",
    "        self.target_tokens = sorted(list(set(self.target_tokens)))\n",
    "        self.num_encoder_tokens = len(self.input_tokens)\n",
    "        self.num_decoder_tokens = len(self.target_tokens)\n",
    "        max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "        max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "        print('Number of samples:', len(input_texts))\n",
    "        print('Number of unique input tokens:', self.num_encoder_tokens)\n",
    "        print('Number of unique output tokens:', self.num_decoder_tokens)\n",
    "        print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "        print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "        \n",
    "        # 3. Replace words with frequency under freq\n",
    "        \n",
    "\n",
    "        input_token_index = dict(\n",
    "            [(token, i) for i, token in enumerate(self.input_tokens)])\n",
    "        target_token_index = dict(\n",
    "            [(token, i) for i, token in enumerate(self.target_tokens)])\n",
    "\n",
    "        encoder_input_data_dict = dict()\n",
    "        decoder_input_data_dict = dict()\n",
    "        decoder_target_data_dict = dict()\n",
    "        for idx, pos in enumerate(range(0, len(input_texts), num_samples)):\n",
    "            next_pos = min(pos + num_samples, len(input_texts))\n",
    "            \n",
    "            if onehot_style:\n",
    "                encoder_input_data = np.zeros(\n",
    "                    (next_pos - pos, self.max_seq_length, self.num_encoder_tokens),\n",
    "                    dtype='float32')\n",
    "                decoder_input_data = np.zeros(\n",
    "                    (next_pos - pos, self.max_seq_length, self.num_decoder_tokens),\n",
    "                    dtype='float32')\n",
    "                decoder_target_data = np.zeros(\n",
    "                    (next_pos - pos, self.max_seq_length, self.num_decoder_tokens),\n",
    "                    dtype='float32')\n",
    "            else:\n",
    "                encoder_input_data = np.zeros(\n",
    "                    shape=(next_pos - pos, self.max_seq_length),\n",
    "                    dtype='float32')\n",
    "                decoder_input_data = np.zeros(\n",
    "                    (next_pos - pos, self.max_seq_length),\n",
    "                    dtype='float32')\n",
    "                decoder_target_data = np.zeros(\n",
    "                    (next_pos - pos, self.max_seq_length),\n",
    "                    dtype='float32')\n",
    "            \n",
    "            batch_input_texts = input_texts[pos: next_pos]\n",
    "            batch_target_texts = target_texts[pos: next_pos]\n",
    "\n",
    "            for i, (input_text, target_text) in enumerate(zip(batch_input_texts, batch_target_texts)):\n",
    "                for t, token in enumerate(input_text):\n",
    "                    if t >= self.max_seq_length:\n",
    "                        break\n",
    "#                     if char not in input_token_index:\n",
    "#                         char = '<unk>'\n",
    "                    if token not in input_token_index:\n",
    "                        put_index = input_token_index['<unk>']\n",
    "                    else:\n",
    "                        put_index = input_token_index[token]\n",
    "            \n",
    "                    if onehot_style:\n",
    "                        encoder_input_data[i, t, put_index] = 1.\n",
    "                    else:\n",
    "                        encoder_input_data[i, t] = put_index\n",
    "\n",
    "                for t, token in enumerate(target_text):\n",
    "                    if t >= self.max_seq_length:\n",
    "                        break\n",
    "                    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "                    if token not in target_token_index:\n",
    "                        put_index = target_token_index['<unk>']\n",
    "                    else:\n",
    "                        put_index = target_token_index[token]\n",
    "                            \n",
    "                    if onehot_style:\n",
    "                        decoder_input_data[i, t, put_index] = 1.\n",
    "                        if t > 0:\n",
    "                            # decoder_target_data will be ahead by one timestep\n",
    "                            # and will not include the start character.\n",
    "                            decoder_target_data[i, t - 1, target_token_index[token]] = 1.\n",
    "                    else:\n",
    "                        decoder_input_data[i, t] = put_index\n",
    "                        if t > 0:\n",
    "                            # decoder_target_data will be ahead by one timestep\n",
    "                            # and will not include the start character.\n",
    "                            decoder_target_data[i, t - 1] = put_index\n",
    "            \n",
    "            if not embedding_version:\n",
    "                new_shape = np.append(encoder_input_data.shape, 1)\n",
    "                encoder_input_data = encoder_input_data.reshape(new_shape)\n",
    "                \n",
    "\n",
    "                new_shape = np.append(decoder_input_data.shape, 1)\n",
    "                decoder_input_data = decoder_input_data.reshape(new_shape)\n",
    "                \n",
    "\n",
    "                new_shape = np.append(decoder_target_data.shape, 1)\n",
    "                decoder_target_data = decoder_target_data.reshape(new_shape)\n",
    "                \n",
    "            encoder_input_data_dict[idx] = encoder_input_data\n",
    "            decoder_input_data_dict[idx] = decoder_input_data\n",
    "            decoder_target_data_dict[idx] = decoder_target_data\n",
    "            \n",
    "        self.input_token_index = input_token_index\n",
    "        self.target_token_index = target_token_index\n",
    "        \n",
    "        # Always dataset comes first then additional info comes next\n",
    "        return [[encoder_input_data_dict, decoder_input_data_dict, decoder_target_data_dict], \n",
    "                [self.num_encoder_tokens, self.num_decoder_tokens],\n",
    "               [input_texts, target_texts]]\n",
    "    \n",
    "    def tokenizer(self, sentence, token_type='char', kor_tokenizer='custom', kor_tokenizer_max_word_char=2):\n",
    "        if token_type == 'char':\n",
    "            return [s for s in sentence]\n",
    "        else: # token_type == word\n",
    "            if kor_tokenizer == 'okt':\n",
    "                return self.okt.morphs(sentence, norm=True, stem=True)\n",
    "            else: # 'custom'\n",
    "                tokenized_sentence = sentence.split(' ')\n",
    "                if kor_tokenizer_max_word_char == -1:\n",
    "                    return [word for word in tokenized_sentence]\n",
    "                else:\n",
    "                    return [word[:kor_tokenizer_max_word_char] for word in tokenized_sentence]\n",
    "    \n",
    "    def get_data(self, data_path_list, data_type='translation', token_type='char',\n",
    "                 kor_tokenizer='custom', kor_tokenizer_max_word_char=2, num_samples=10000):\n",
    "        data_type_dict = {\n",
    "            'translation': self._read_data_seq2seq\n",
    "        }\n",
    "        \n",
    "        if data_type not in data_type_dict:\n",
    "            print('not valid data type. given: translation')\n",
    "        \n",
    "        all_data = data_type_dict[data_type](data_path_list=data_path_list, token_type=token_type, \n",
    "                                             kor_tokenizer=kor_tokenizer, kor_tokenizer_max_word_char=kor_tokenizer_max_word_char,\n",
    "                                             num_samples=num_samples)\n",
    "        \n",
    "        return all_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 94124\n",
      "Number of unique input tokens: 55605\n",
      "Number of unique output tokens: 52084\n",
      "Max sequence length for inputs: 103\n",
      "Max sequence length for outputs: 120\n",
      "Number of samples: 1001\n",
      "Number of unique input tokens: 55934\n",
      "Number of unique output tokens: 52442\n",
      "Max sequence length for inputs: 77\n",
      "Max sequence length for outputs: 75\n",
      "\n",
      "Number of samples: 2001\n",
      "Number of unique input tokens: 56692\n",
      "Number of unique output tokens: 53115\n",
      "Max sequence length for inputs: 81\n",
      "Max sequence length for outputs: 97\n"
     ]
    }
   ],
   "source": [
    "# This is only for test\n",
    "num_samples = 50000\n",
    "dataset = Dataset(max_seq_length=50)\n",
    "train_data = dataset.get_data(data_path_list=[input_data_path, output_data_path], data_type='translation', \n",
    "                              token_type='word', kor_tokenizer='okt', kor_tokenizer_max_word_char=2, num_samples=num_samples)\n",
    "# print('')\n",
    "val_data = dataset.get_data(data_path_list=[val_input_data_path, val_output_data_path], data_type='translation', \n",
    "                            token_type='word', kor_tokenizer='okt', kor_tokenizer_max_word_char=2, num_samples=num_samples)\n",
    "print('')\n",
    "test_data = dataset.get_data(data_path_list=[test_input_data_path, test_output_data_path], data_type='translation', \n",
    "                             token_type='word', kor_tokenizer='okt', kor_tokenizer_max_word_char=2, num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Encoder & Decoder inputs and Decoder output as onehot (original form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector, Lambda, TimeDistributed, Reshape, Concatenate, Permute, Multiply, GRU, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding version: size x sequence_length, onehot batch conversion for the output\n"
     ]
    }
   ],
   "source": [
    "print('Embedding version: size x sequence_length, onehot batch conversion for the output')\n",
    "\n",
    "def RepeatVectorLayer(rep, axis):\n",
    "    return Lambda(lambda x: K.repeat_elements(K.expand_dims(x, axis), rep, axis),\n",
    "                         lambda x: tuple((x[0],) + x[1:axis] + (rep,) + x[axis:]))\n",
    "\n",
    "def convert_generator(x_data, y_data, num_encoder_tokens, num_decoder_tokens, batch_size=16):\n",
    "    '''\n",
    "    Return a random from x_data, y_data\n",
    "    '''\n",
    "    while True:\n",
    "        for me_idx in x_data[0].keys():\n",
    "            samples_per_mini_epoch = x_data[0][me_idx].shape[0]\n",
    "            number_of_steps = np.ceil(samples_per_mini_epoch / batch_size).astype(int)\n",
    "            data_idx = list(range(number_of_steps))\n",
    "        \n",
    "            while len(data_idx) > 0:\n",
    "                # choose batch_size random images / labels from the data\n",
    "                \n",
    "                idx = random.choice(data_idx)\n",
    "                next_idx = min(samples_per_mini_epoch, idx + batch_size)\n",
    "                encoder_input = x_data[0][me_idx][idx: next_idx]\n",
    "                decoder_input = x_data[1][me_idx][idx: next_idx]\n",
    "                decoder_output = y_data[me_idx][idx: next_idx]\n",
    "                \n",
    "                converted_encoder_input = np.zeros(shape=(encoder_input.shape[0], encoder_input.shape[1], num_encoder_tokens))\n",
    "                for idx2, data in enumerate(encoder_input):\n",
    "                    for idx3, d in enumerate(data):\n",
    "                        converted_encoder_input[idx2, idx3, int(d)] = 1\n",
    "                        \n",
    "                converted_decoder_input = np.zeros(shape=(decoder_input.shape[0], decoder_input.shape[1], num_decoder_tokens))\n",
    "                for idx2, data in enumerate(decoder_input):\n",
    "                    for idx3, d in enumerate(data):\n",
    "                        converted_decoder_input[idx2, idx3, int(d)] = 1\n",
    "                \n",
    "                converted_decoder_output = np.zeros(shape=(decoder_output.shape[0], decoder_output.shape[1], num_decoder_tokens))\n",
    "                for idx2, data in enumerate(decoder_output):\n",
    "                    for idx3, d in enumerate(data):\n",
    "                        converted_decoder_output[idx2, idx3, int(d)] = 1\n",
    "\n",
    "                data_idx.remove(idx)\n",
    "                yield [converted_encoder_input, converted_decoder_input], converted_decoder_output\n",
    "\n",
    "class Sequence2sequence:\n",
    "    def __init__(self, initial_params, gpu=0):\n",
    "        \n",
    "        self.max_encoder_seq_length, self.max_decoder_seq_length, self.num_encoder_tokens, self.num_decoder_tokens = initial_params\n",
    "#         print('num_encoder_tokens: %i, num_decoder_tokens: %i' %( self.num_encoder_tokens, self.num_decoder_tokens))\n",
    "        \n",
    "        self.embedding_dim = 256\n",
    "        self.latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "        self.max_enc_seq_length = 50\n",
    "        self.max_dec_seq_length = 50\n",
    "        \n",
    "        with K.tf.device('/gpu:' + str(gpu)):\n",
    "            # Define an input sequence and process it.\n",
    "            encoder_inputs = Input(shape=(self.max_enc_seq_length, self.num_encoder_tokens))\n",
    "           \n",
    "            encoder = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n",
    "            encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "            # We discard `encoder_outputs` and only keep the states.\n",
    "            encoder_states = [state_h, state_c]\n",
    "\n",
    "            # Set up the decoder, using `encoder_states` as initial state.\n",
    "            decoder_inputs = Input(shape=(self.max_dec_seq_length, self.num_decoder_tokens))\n",
    "            # We set up our decoder to return full output sequences,\n",
    "            # and to return internal states as well. We don't use the\n",
    "            # return states in the training model, but we will use them in inference.\n",
    "            decoder = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n",
    "            decoder_outputs, _, _ = decoder(decoder_inputs,\n",
    "                                                 initial_state=encoder_states)\n",
    "            \n",
    "#             encoder_inputs = Input(shape=(self.max_enc_seq_length, self.num_encoder_tokens))\n",
    "#             encoder = GRU(self.latent_dim, return_sequences=True, return_state=True)\n",
    "#             encoder_outputs, encoder_states = encoder(encoder_inputs)\n",
    "            \n",
    "#             decoder_inputs = Input(shape=(self.max_dec_seq_length, self.num_decoder_tokens))\n",
    "#             decoder = GRU(self.latent_dim, return_sequences=True, return_state=True)\n",
    "#             decoder_outputs, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
    "            \n",
    "            \n",
    "            # 어텐션 매커니즘.\n",
    "            repeat_d_layer = RepeatVectorLayer(self.max_enc_seq_length, 2)\n",
    "            repeat_d = repeat_d_layer(decoder_outputs)\n",
    "\n",
    "            repeat_e_layer = RepeatVectorLayer(self.max_dec_seq_length, 1)\n",
    "            repeat_e = repeat_e_layer(encoder_outputs)\n",
    "\n",
    "            concat_for_score_layer = Concatenate(axis=-1)\n",
    "            concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
    "\n",
    "            dense1_t_score_layer = Dense(self.latent_dim // 2, activation='tanh')\n",
    "            dense1_score_layer = TimeDistributed(dense1_t_score_layer)\n",
    "            dense1_score = dense1_score_layer(concat_for_score)\n",
    "            dense2_t_score_layer = Dense(1) # to make softmax comparison\n",
    "            dense2_score_layer = TimeDistributed(dense2_t_score_layer)\n",
    "            dense2_score = dense2_score_layer(dense1_score)\n",
    "            dense2_score = Reshape((self.max_dec_seq_length, self.max_enc_seq_length))(dense2_score) # reshape to be 2 dims\n",
    "\n",
    "            softmax_score_layer = Softmax(axis=-1)\n",
    "            softmax_score = softmax_score_layer(dense2_score)\n",
    "\n",
    "            repeat_score_layer = RepeatVectorLayer(self.latent_dim, 2)\n",
    "            repeat_score = repeat_score_layer(softmax_score)\n",
    "\n",
    "            permute_e = Permute((2, 1))(encoder_outputs)\n",
    "            repeat_e_layer = RepeatVectorLayer(self.max_dec_seq_length, 1)\n",
    "            repeat_e = repeat_e_layer(permute_e)\n",
    "\n",
    "            attended_mat_layer = Multiply()\n",
    "            attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
    "\n",
    "            context_layer = Lambda(lambda x: K.sum(x, axis=-1),\n",
    "                                         lambda x: tuple(x[:-1]))\n",
    "            context = context_layer(attended_mat)\n",
    "\n",
    "            concat_context_layer = Concatenate(axis=-1)\n",
    "            concat_context = concat_context_layer([context, decoder_outputs])\n",
    "\n",
    "            attention_dense_output_layer = Dense(self.latent_dim, activation='tanh')\n",
    "            attention_output_layer = TimeDistributed(attention_dense_output_layer)\n",
    "            attention_output = attention_output_layer(concat_context)\n",
    "            \n",
    "            \n",
    "            decoder_dense = Dense(self.num_decoder_tokens, activation='softmax')\n",
    "            decoder_outputs_pred = decoder_dense(attention_output)\n",
    "            \n",
    "        # Define the model that will turn\n",
    "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs_pred)\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.encoder_states = encoder_states\n",
    "        self.encoder_outputs = encoder_outputs\n",
    "        self.decoder = decoder\n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        self.decoder_dense = decoder_dense\n",
    "        self.repeat_d_layer = repeat_d_layer\n",
    "        self.repeat_e_layer = repeat_e_layer\n",
    "        self.attention_output = attention_output\n",
    "        \n",
    "\n",
    "        # Run training\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "        print(model.summary())\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def train(self, train_data, validation_data=None, epochs=50, batch_size=64, verbose=1, validation_split=0.1, *args):\n",
    "#         print(len(args))\n",
    "        encinput_train_dict, decinput_train_dict, dectarget_train_dict = train_data\n",
    "        if validation_data is not None:\n",
    "            encinput_val_dict, decinput_val_dict, dectarget_val_dict = validation_data\n",
    "        \n",
    "        mc = ModelCheckpoint('./save/s2s_{epoch:03d}.h5', save_weights_only=True, period=5)\n",
    "        val_steps = 0\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            val_data = ([encinput_val_dict, decinput_val_dict], dectarget_val_dict)\n",
    "            for encinput_val_data in encinput_val_dict.values():\n",
    "                val_steps += np.ceil(encinput_val_data.shape[0] / batch_size)\n",
    "            val_data = convert_generator(x_data=val_data[0], y_data=val_data[1], batch_size=batch_size,\n",
    "                                        num_decoder_tokens=self.num_decoder_tokens,\n",
    "                                        num_encoder_tokens=self.num_encoder_tokens)\n",
    "        else:\n",
    "            val_data = None\n",
    "\n",
    "        steps_per_epoch = 0\n",
    "        for encinput_train_data in encinput_train_dict.values():\n",
    "                steps_per_epoch += np.ceil(encinput_train_data.shape[0] / batch_size)\n",
    "                \n",
    "        self.model.fit_generator(generator=convert_generator([encinput_train_dict, decinput_train_dict],\n",
    "                                                            dectarget_train_dict,\n",
    "                                                            num_decoder_tokens=self.num_decoder_tokens,\n",
    "                                                            num_encoder_tokens=self.num_encoder_tokens),\n",
    "                                 steps_per_epoch=steps_per_epoch,\n",
    "                                 validation_data=val_data,\n",
    "                                 validation_steps=val_steps,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=verbose,\n",
    "                                 callbacks=[mc]\n",
    "                                )\n",
    "        print('')\n",
    "            \n",
    "            \n",
    "    def test(self, test_data, batch_size=64):\n",
    "        print('')\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_initial_params(*args):\n",
    "#         print(len(args))\n",
    "#         print(len(args[0]))\n",
    "        (encinput_train_dict, decinput_train_dict, dectarget_train_dict), (num_encoder_tokens, num_decoder_tokens), _ = args[0]\n",
    "        encoder_input_data, decoder_input_data, decoder_target_data = encinput_train_dict[0], decinput_train_dict[0], dectarget_train_dict[0]\n",
    "        return [encoder_input_data.shape[1], decoder_input_data.shape[1], num_encoder_tokens, num_decoder_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50, 56692)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 50, 53115)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 58315776    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 54652928    input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 50, 50, 256)  0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 50, 50, 256)  0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50, 50, 512)  0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 50, 50, 128)  65664       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 50, 50, 1)    129         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 50, 50)       0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 50, 50)       0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 256, 50)      0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 50, 256, 50)  0           softmax_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 50, 256, 50)  0           permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 50, 256, 50)  0           lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 50, 256)      0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 50, 512)      0           lambda_5[0][0]                   \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 50, 256)      131328      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 50, 53115)    13650555    time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 126,816,380\n",
      "Trainable params: 126,816,380\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 8571s 1s/step - loss: 1.9632 - val_loss: 7.8524\n",
      "Epoch 2/10\n",
      " 427/5883 [=>............................] - ETA: 2:11:39 - loss: 2.6651"
     ]
    }
   ],
   "source": [
    "seq2seq_try11 = Sequence2sequence(Sequence2sequence.get_initial_params(test_data), gpu=0)\n",
    "seq2seq_try11.train(train_data=train_data[0], \n",
    "              validation_data=val_data[0], \n",
    "              epochs=10, batch_size=16, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 1, 100, 256), (None, 100, 100, 256)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7f250fccef63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mrepeat_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat_e_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mconcat_for_score_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mconcat_for_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_for_score_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepeat_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat_e\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mdense1_score_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1_t_score_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mdense1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense1_score_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_for_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m                              \u001b[0;34m'inputs with matching shapes '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                              \u001b[0;34m'except for the concat axis. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                              'Got inputs shapes: %s' % (input_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 1, 100, 256), (None, 100, 100, 256)]"
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "target_data = test_data\n",
    "\n",
    "# Define sampling models\n",
    "\n",
    "# added for my version\n",
    "encoder_inputs = seq2seq_try11.encoder_inputs\n",
    "encoder_states = seq2seq_try11.encoder_states\n",
    "latent_dim = seq2seq_try11.latent_dim\n",
    "max_encoder_seq_length = seq2seq_try11.max_encoder_seq_length\n",
    "max_decoder_seq_length = seq2seq_try11.max_decoder_seq_length\n",
    "\n",
    "decoder = seq2seq_try11.decoder\n",
    "decoder_inputs = seq2seq_try11.decoder_inputs\n",
    "decoder_dense = seq2seq_try11.decoder_dense\n",
    "repeat_d_layer = seq2seq_try11.repeat_d_layer\n",
    "repeat_e_layer = seq2seq_try11.repeat_e_layer\n",
    "attention_output = seq2seq_try11.attention_output\n",
    "encoder_outputs = seq2seq_try11.encoder_outputs\n",
    "\n",
    "input_token_index = dict(\n",
    "            [(token, i) for i, token in enumerate(dataset.input_tokens)])\n",
    "target_token_index = dict(\n",
    "    [(token, i) for i, token in enumerate(dataset.target_tokens)])\n",
    "\n",
    "(encinput_train_dict, decinput_train_dict, dectarget_train_dict), (num_encoder_tokens, num_decoder_tokens), (input_texts, target_texts) = target_data\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = encinput_train_dict[0], decinput_train_dict[0], dectarget_train_dict[0]\n",
    "\n",
    "\n",
    "\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states[0], encoder_states[1]]) \n",
    "encoder_outputs_input = Input(shape=(max_encoder_seq_length, latent_dim)) \n",
    "decoder_inputs = Input(shape=(1, num_decoder_tokens)) \n",
    "decoder_state_input_h = Input(shape=(latent_dim,)) \n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, decoder_h, decoder_c = decoder(decoder_inputs, initial_state=decoder_states_inputs) \n",
    "\n",
    "# repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2) \n",
    "repeat_d = repeat_d_layer(decoder_outputs) \n",
    "# repeat_e_layer = RepeatVectorLayer(1, axis=1) \n",
    "repeat_e = repeat_e_layer(encoder_outputs_input) \n",
    "concat_for_score_layer = Concatenate(axis=-1) \n",
    "concat_for_score = concat_for_score_layer([repeat_d, repeat_e]) \n",
    "dense1_score_layer = TimeDistributed(dense1_t_score_layer) \n",
    "dense1_score = dense1_score_layer(concat_for_score) \n",
    "dense2_score_layer = TimeDistributed(dense2_t_score_layer) \n",
    "dense2_score = dense2_score_layer(dense1_score) \n",
    "dense2_score = Reshape((1, max_encoder_seq_length))(dense2_score)\n",
    "softmax_score_layer = Softmax(axis=-1) \n",
    "softmax_score = softmax_score_layer(dense2_score) \n",
    "repeat_score_layer = RepeatVectorLayer(latent_dim, 2) \n",
    "repeat_score = repeat_score_layer(softmax_score) \n",
    "permute_e = Permute((2, 1))(encoder_outputs_input) \n",
    "repeat_e_layer = RepeatVectorLayer(1, axis=1) \n",
    "repeat_e = repeat_e_layer(permute_e) \n",
    "attended_mat_layer = Multiply() \n",
    "attended_mat = attended_mat_layer([repeat_score, repeat_e]) \n",
    "context_layer = Lambda(lambda x: K.sum(x, axis=-1), lambda x: tuple(x[:-1])) \n",
    "context = context_layer(attended_mat) \n",
    "concat_context_layer = Concatenate(axis=-1) \n",
    "concat_context = concat_context_layer([context, decoder_outputs]) \n",
    "attention_output_layer = TimeDistributed(attention_dense_output_layer)\n",
    "attention_output = attention_output_layer(concat_context) \n",
    "decoder_att_outputs = decoder_dense(attention_output) \n",
    "decoder_model = Model([decoder_inputs, decoder_state_input_h, decoder_state_input_c, encoder_outputs_input], \n",
    "                      [decoder_outputs, decoder_h, decoder_c, decoder_att_outputs])\n",
    "\n",
    "\n",
    "# encoder_model = Model(encoder_inputs, encoder_states)\n",
    "# # LSTM\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "#     decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# decoder_states = [state_h, state_c]\n",
    "\n",
    "# # GRU\n",
    "# # decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# # decoder_states_inputs = [decoder_state_input_h]\n",
    "# # decoder_outputs, state_h = decoder(\n",
    "# #     decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# # decoder_states = [state_h]\n",
    "\n",
    "# encoder_outputs_input = Input(shape=(max_encoder_seq_length, latent_dim))\n",
    "# m = Model(inputs=[encoder_outputs_input, decoder_outputs], )\n",
    "\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs,\n",
    "#     [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    _input_seq = input_seq\n",
    "    input_seq = np.zeros(shape=(input_seq.shape[0], input_seq.shape[1], seq2seq_try11.num_encoder_tokens))\n",
    "    for idx2, data in enumerate(_input_seq):\n",
    "        for idx3, d in enumerate(data):\n",
    "            input_seq[idx2, idx3, int(d)] = 1\n",
    "    \n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "#         output_tokens, h, c = decoder_model.predict(\n",
    "#             [target_seq] + states_value)\n",
    "        output_tokens, h = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > dataset.max_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "#         states_value = [h, c]\n",
    "        states_value = h\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    print('Target sentence:', target_texts[seq_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encoder & Decoder inputs as indices and decoder output as onehot, Encoder & Decoder inputs will be embeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding version: size x sequence_length, onehot batch conversion for the output\n"
     ]
    }
   ],
   "source": [
    "print('Embedding version: size x sequence_length, onehot batch conversion for the output')\n",
    "\n",
    "def convert_generator(x_data, y_data, num_decoder_tokens, batch_size=16):\n",
    "    '''\n",
    "    Return a random from x_data, y_data\n",
    "    '''\n",
    "    while True:\n",
    "        for me_idx in x_data[0].keys():\n",
    "            samples_per_mini_epoch = x_data[0][me_idx].shape[0]\n",
    "            number_of_steps = np.ceil(samples_per_mini_epoch / batch_size).astype(int)\n",
    "            data_idx = list(range(number_of_steps))\n",
    "        \n",
    "            while len(data_idx) > 0:\n",
    "                # choose batch_size random images / labels from the data\n",
    "                \n",
    "                idx = random.choice(data_idx)\n",
    "                next_idx = min(samples_per_mini_epoch, idx + batch_size)\n",
    "                encoder_input = x_data[0][me_idx][idx: next_idx]\n",
    "                decoder_input = x_data[1][me_idx][idx: next_idx]\n",
    "                decoder_output = y_data[me_idx][idx: next_idx]\n",
    "                converted_decoder_output = np.zeros(shape=(decoder_output.shape[0], decoder_output.shape[1], num_decoder_tokens))\n",
    "                for idx2, data in enumerate(decoder_output):\n",
    "                    for idx3, d in enumerate(data):\n",
    "                        converted_decoder_output[idx2, idx3, int(d)] = 1\n",
    "\n",
    "                data_idx.remove(idx)\n",
    "                yield [encoder_input, decoder_input], converted_decoder_output\n",
    "\n",
    "class Sequence2sequence:\n",
    "    def __init__(self, initial_params, gpu=0):\n",
    "        \n",
    "        self.max_encoder_seq_length, self.max_decoder_seq_length, self.num_encoder_tokens, self.num_decoder_tokens = initial_params\n",
    "        \n",
    "        self.embedding_dim = 256\n",
    "        self.latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "        self.max_enc_seq_length = 50\n",
    "        self.max_dec_seq_length = 50\n",
    "        \n",
    "        with K.tf.device('/gpu:' + str(gpu)):\n",
    "            # Define an input sequence and process it.\n",
    "            encoder_inputs = Input(shape=(self.max_enc_seq_length, ))\n",
    "            embeded_encoder_inputs = Embedding(self.num_encoder_tokens, self.embedding_dim, \n",
    "                                               embeddings_constraint=MinMaxNorm(min_value=0.0, max_value=1.0, axis=1),\n",
    "                                               embeddings_initializer=RandomUniform(minval=0.0, maxval=1.0), \n",
    "                                               input_length=self.max_encoder_seq_length)(encoder_inputs)\n",
    "#             embeded_encoder_inputs = Activation('tanh')(embeded_encoder_inputs)\n",
    "            encoder = LSTM(self.latent_dim, return_state=True)\n",
    "            encoder_outputs, state_h, state_c = encoder(embeded_encoder_inputs)\n",
    "            # We discard `encoder_outputs` and only keep the states.\n",
    "            encoder_states = [state_h, state_c]\n",
    "\n",
    "            # Set up the decoder, using `encoder_states` as initial state.\n",
    "            decoder_inputs = Input(shape=(self.max_dec_seq_length, ))\n",
    "            decoder_embed = Embedding(self.num_decoder_tokens, self.embedding_dim, \n",
    "#                                       embeddings_constraint=MinMaxNorm(min_value=0.0, max_value=1.0, axis=1),\n",
    "#                                       embeddings_initializer=RandomUniform(minval=0.0, maxval=1.0), \n",
    "                                      input_length=self.max_decoder_seq_length)\n",
    "            embeded_decoder_inputs = decoder_embed(decoder_inputs)\n",
    "#             embeded_decoder_inputs = Activation('tanh')(embeded_decoder_inputs)\n",
    "            # We set up our decoder to return full output sequences,\n",
    "            # and to return internal states as well. We don't use the\n",
    "            # return states in the training model, but we will use them in inference.\n",
    "            decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n",
    "            decoder_outputs, _, _ = decoder_lstm(embeded_decoder_inputs,\n",
    "                                                 initial_state=encoder_states)\n",
    "            decoder_dense = Dense(self.num_decoder_tokens, activation='softmax')\n",
    "            decoder_outputs_pred = decoder_dense(decoder_outputs)\n",
    "            \n",
    "\n",
    "        # Define the model that will turn\n",
    "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs_pred)\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.encoder_states = encoder_states\n",
    "        self.decoder_lstm = decoder_lstm\n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        self.decoder_dense = decoder_dense\n",
    "        self.decoder_embed = decoder_embed\n",
    "        self.embeded_decoder_inputs = embeded_decoder_inputs\n",
    "\n",
    "        # Run training\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "        print(model.summary())\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def train(self, train_data, validation_data=None, epochs=50, batch_size=64, verbose=1, validation_split=0.1, *args):\n",
    "#         print(len(args))\n",
    "        encinput_train_dict, decinput_train_dict, dectarget_train_dict = train_data\n",
    "        if validation_data is not None:\n",
    "            encinput_val_dict, decinput_val_dict, dectarget_val_dict = validation_data\n",
    "        \n",
    "        mc = ModelCheckpoint('./save/s2s_{epoch:03d}.h5', save_weights_only=True, period=5)\n",
    "        val_steps = 0\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            val_data = ([encinput_val_dict, decinput_val_dict], dectarget_val_dict)\n",
    "            for encinput_val_data in encinput_val_dict.values():\n",
    "                val_steps += np.ceil(encinput_val_data.shape[0] / batch_size)\n",
    "            val_data = convert_generator(x_data=val_data[0], y_data=val_data[1], batch_size=batch_size,\n",
    "                                        num_decoder_tokens=self.num_decoder_tokens)\n",
    "        else:\n",
    "            val_data = None\n",
    "\n",
    "        steps_per_epoch = 0\n",
    "        for encinput_train_data in encinput_train_dict.values():\n",
    "                steps_per_epoch += np.ceil(encinput_train_data.shape[0] / batch_size)\n",
    "                \n",
    "        self.model.fit_generator(generator=convert_generator([encinput_train_dict, decinput_train_dict],\n",
    "                                                            dectarget_train_dict,\n",
    "                                                            num_decoder_tokens=self.num_decoder_tokens),\n",
    "                                 steps_per_epoch=steps_per_epoch,\n",
    "                                 validation_data=val_data,\n",
    "                                 validation_steps=val_steps,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=verbose,\n",
    "                                 callbacks=[mc]\n",
    "                                )\n",
    "        print('')\n",
    "            \n",
    "            \n",
    "    def test(self, test_data, batch_size=64):\n",
    "        print('')\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_initial_params(*args):\n",
    "#         print(len(args))\n",
    "#         print(len(args[0]))\n",
    "        (encinput_train_dict, decinput_train_dict, dectarget_train_dict), (num_encoder_tokens, num_decoder_tokens), _ = args[0]\n",
    "        encoder_input_data, decoder_input_data, decoder_target_data = encinput_train_dict[0], decinput_train_dict[0], dectarget_train_dict[0]\n",
    "        return [encoder_input_data.shape[1], decoder_input_data.shape[1], num_encoder_tokens, num_decoder_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 256)      14513152    input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 50, 256)      13597440    input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 256), (None, 525312      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, 50, 256), (N 525312      embedding_2[0][0]                \n",
      "                                                                 lstm_9[0][1]                     \n",
      "                                                                 lstm_9[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 50, 53115)    13650555    lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 42,811,771\n",
      "Trainable params: 42,811,771\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "11766/11766 [==============================] - 4011s 341ms/step - loss: 1.6116 - val_loss: 8.2593\n",
      "Epoch 2/50\n",
      "11766/11766 [==============================] - 4012s 341ms/step - loss: 0.9331 - val_loss: 7.6999\n",
      "Epoch 3/50\n",
      "11766/11766 [==============================] - 4020s 342ms/step - loss: 0.6786 - val_loss: 7.6881\n",
      "Epoch 4/50\n",
      "11766/11766 [==============================] - 4022s 342ms/step - loss: 0.5243 - val_loss: 7.9150\n",
      "Epoch 5/50\n",
      "11766/11766 [==============================] - 4023s 342ms/step - loss: 0.4132 - val_loss: 8.0641\n",
      "Epoch 6/50\n",
      "11766/11766 [==============================] - 4012s 341ms/step - loss: 0.3339 - val_loss: 8.2012\n",
      "Epoch 7/50\n",
      "11766/11766 [==============================] - 4013s 341ms/step - loss: 0.2761 - val_loss: 8.3051\n",
      "Epoch 8/50\n",
      "11766/11766 [==============================] - 4015s 341ms/step - loss: 0.2362 - val_loss: 8.3252\n",
      "Epoch 9/50\n",
      "11766/11766 [==============================] - 4015s 341ms/step - loss: 0.2041 - val_loss: 8.3333\n",
      "Epoch 10/50\n",
      "11766/11766 [==============================] - 4016s 341ms/step - loss: 0.1773 - val_loss: 8.4629\n",
      "Epoch 11/50\n",
      "11766/11766 [==============================] - 4011s 341ms/step - loss: 0.1582 - val_loss: 8.4299\n",
      "Epoch 12/50\n",
      "11766/11766 [==============================] - 4014s 341ms/step - loss: 0.1441 - val_loss: 8.3812\n",
      "Epoch 13/50\n",
      "11766/11766 [==============================] - 4020s 342ms/step - loss: 0.1303 - val_loss: 8.4802\n",
      "Epoch 14/50\n",
      "11766/11766 [==============================] - 4023s 342ms/step - loss: 0.1197 - val_loss: 8.3982\n",
      "Epoch 15/50\n",
      "11766/11766 [==============================] - 4034s 343ms/step - loss: 0.1108 - val_loss: 8.5284\n",
      "Epoch 16/50\n",
      " 9437/11766 [=======================>......] - ETA: 13:11 - loss: 0.1286"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1fc007a3afc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m seq2seq_try21.train(train_data=train_data[0], \n\u001b[1;32m      3\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m               epochs=50, batch_size=8, verbose=1, validation_split=0.1)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-31a269381e9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, validation_data, epochs, batch_size, verbose, validation_split, *args)\u001b[0m\n\u001b[1;32m    115\u001b[0m                                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                                  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                                  \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                                 )\n\u001b[1;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq2seq_try21 = Sequence2sequence(Sequence2sequence.get_initial_params(test_data), gpu=0)\n",
    "seq2seq_try21.train(train_data=train_data[0], \n",
    "              validation_data=val_data[0], \n",
    "              epochs=50, batch_size=8, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_10: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-743c552c911f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdecoder_states_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecoder_state_input_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m decoder_outputs, state_h, state_c = decoder_lstm(\n\u001b[0;32m---> 35\u001b[0;31m     decoder_inputs, initial_state=decoder_states_inputs)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mdecoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0moriginal_input_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# with the input_spec set at build time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Handle mask propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/virtualenv/brainwave_eeg/venv/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_10: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "target_data = test_data\n",
    "\n",
    "# Define sampling models\n",
    "\n",
    "# added for my version\n",
    "encoder_inputs = seq2seq_try21.encoder_inputs\n",
    "encoder_states = seq2seq_try21.encoder_states\n",
    "latent_dim = seq2seq_try21.latent_dim\n",
    "decoder_lstm = seq2seq_try21.decoder_lstm\n",
    "decoder_inputs = seq2seq_try21.decoder_inputs\n",
    "decoder_dense = seq2seq_try21.decoder_dense\n",
    "input_token_index = dict(\n",
    "            [(token, i) for i, token in enumerate(dataset.input_tokens)])\n",
    "target_token_index = dict(\n",
    "    [(token, i) for i, token in enumerate(dataset.target_tokens)])\n",
    "\n",
    "(encinput_train_dict, decinput_train_dict, dectarget_train_dict), (num_encoder_tokens, num_decoder_tokens), (input_texts, target_texts) = target_data\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = encinput_train_dict[0], decinput_train_dict[0], dectarget_train_dict[0]\n",
    "\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    _input_seq = input_seq\n",
    "    input_seq = np.zeros(shape=(input_seq.shape[0], input_seq.shape[1], seq2seq_try11.num_encoder_tokens))\n",
    "    for idx2, data in enumerate(_input_seq):\n",
    "        for idx3, d in enumerate(data):\n",
    "            input_seq[idx2, idx3, int(d)] = 1\n",
    "    \n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > dataset.max_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    print('Target sentence:', target_texts[seq_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Embedding to Embedding (All inputs of encoder & decoder, and output of decoder will be fed as indices of character tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 1-1. Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Embedding to Embedding')\n",
    "def convert_generator(x_data, y_data, num_decoder_tokens, batch_size=16):\n",
    "    '''\n",
    "    Return a random image from x_data, y_data\n",
    "    '''\n",
    "    \n",
    "    samples_per_epoch = x_data[0].shape[0]\n",
    "    number_of_steps = np.ceil(samples_per_epoch / batch_size).astype(int)\n",
    "    data_idx = list(range(number_of_steps))\n",
    "#     print('number of unique decoder tokens:', num_decoder_tokens)\n",
    "    \n",
    "    while True:\n",
    "        # choose batch_size random images / labels from the data\n",
    "#         idx = np.random.randint(0, x_data.shape[0], batch_size)\n",
    "        idx = random.choice(data_idx)\n",
    "        next_idx = min(samples_per_epoch, idx + batch_size)\n",
    "        encoder_input = x_data[0][idx: next_idx]\n",
    "        decoder_input = x_data[1][idx: next_idx]\n",
    "        decoder_output = y_data[idx: next_idx]\n",
    "        converted_decoder_output = np.zeros(shape=(decoder_output.shape[0], decoder_output.shape[1], num_decoder_tokens))\n",
    "        for idx2, data in enumerate(decoder_output):\n",
    "            for idx3, d in enumerate(data):\n",
    "                converted_decoder_output[idx2, idx3, int(d)] = 1\n",
    "        \n",
    "        data_idx.remove(idx)\n",
    "        \n",
    "        yield [encoder_input, decoder_input], converted_decoder_output\n",
    "\n",
    "class Sequence2sequence:\n",
    "    def __init__(self, initial_params, options=dict(), gpu=0):\n",
    "        \n",
    "        max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens = initial_params\n",
    "        if 'loss' in options:\n",
    "            loss_type = options['loss']\n",
    "        else:\n",
    "            loss_type = 'categorical_crossentropy'\n",
    "            \n",
    "        loss_dict = {\n",
    "            'categorical_crossentropy': losses.categorical_crossentropy,\n",
    "            'binary_crossentropy': losses.binary_crossentropy,\n",
    "            'mse': losses.mse\n",
    "        }\n",
    "        \n",
    "        loss = loss_dict[loss_type]\n",
    "        \n",
    "        \n",
    "        self.embedding_dim = 256\n",
    "        self.latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "        \n",
    "        with K.tf.device('/gpu:' + str(gpu)):\n",
    "            # Define an input sequence and process it.\n",
    "            encoder_inputs = Input(shape=(None, ))\n",
    "            embeded_encoder_inputs = Embedding(num_encoder_tokens, self.embedding_dim, \n",
    "                                               embeddings_constraint=MinMaxNorm(min_value=0.0, max_value=1.0, axis=1),\n",
    "                                               embeddings_initializer=RandomUniform(minval=0.0, maxval=1.0), \n",
    "                                               input_length=max_encoder_seq_length)(encoder_inputs)\n",
    "            embeded_encoder_inputs = Activation('tanh')(embeded_encoder_inputs)\n",
    "            encoder = LSTM(self.latent_dim, return_state=True)\n",
    "            encoder_outputs, state_h, state_c = encoder(embeded_encoder_inputs)\n",
    "            # We discard `encoder_outputs` and only keep the states.\n",
    "            encoder_states = [state_h, state_c]\n",
    "\n",
    "            # Set up the decoder, using `encoder_states` as initial state.\n",
    "            decoder_inputs = Input(shape=(None, ))\n",
    "            decoder_embed = Embedding(num_decoder_tokens, self.embedding_dim, \n",
    "#                                       embeddings_constraint=MinMaxNorm(min_value=0.0, max_value=1.0, axis=1),\n",
    "#                                       embeddings_initializer=RandomUniform(minval=0.0, maxval=1.0), \n",
    "                                      input_length=max_decoder_seq_length)\n",
    "            embeded_decoder_inputs = decoder_embed(decoder_inputs)\n",
    "            embeded_decoder_inputs = Activation('tanh')(embeded_decoder_inputs)\n",
    "            # We set up our decoder to return full output sequences,\n",
    "            # and to return internal states as well. We don't use the\n",
    "            # return states in the training model, but we will use them in inference.\n",
    "            decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n",
    "            decoder_outputs, _, _ = decoder_lstm(embeded_decoder_inputs,\n",
    "                                                 initial_state=encoder_states)\n",
    "            decoder_dense = Dense(self.embedding_dim)\n",
    "            decoder_outputs_pred = decoder_dense(decoder_outputs)\n",
    "            decoder_outputs_pred = Activation('tanh')(decoder_outputs_pred)\n",
    "            \n",
    "            decoder_outputs_true = Input(shape=(None, ))\n",
    "            embeded_decoder_outputs = decoder_embed(decoder_outputs_true)\n",
    "            embeded_decoder_outputs = Activation('tanh')(embeded_decoder_outputs)\n",
    "\n",
    "        # Define the model that will turn\n",
    "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "        model = Model([encoder_inputs, decoder_inputs, decoder_outputs_true], decoder_outputs_pred)\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.encoder_states = encoder_states\n",
    "        self.decoder_lstm = decoder_lstm\n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        self.decoder_dense = decoder_dense\n",
    "        self.decoder_embed = decoder_embed\n",
    "        self.embeded_decoder_inputs = embeded_decoder_inputs\n",
    "\n",
    "        # Run training\n",
    "        _loss = loss(y_true=embeded_decoder_outputs, y_pred=decoder_outputs_pred)\n",
    "        model.add_loss(_loss) \n",
    "        model.compile(optimizer='rmsprop')\n",
    "        print(model.summary())\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def train(self, train_data, validation_data=None, epochs=50, batch_size=64, verbose=1, validation_split=0.1, *args):\n",
    "#         print(len(args))\n",
    "        encinput_train_dict, decinput_train_dict, dectarget_train_dict = train_data\n",
    "        if validation_data is not None:\n",
    "            encinput_val_dict, decinput_val_dict, dectarget_val_dict = validation_data\n",
    "        \n",
    "        decoder_predicter = Model(inputs=self.decoder_inputs, outputs=self.embeded_decoder_inputs)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print('[EPOCH %i]' % epoch)\n",
    "            for idx in encinput_train_dict.keys():\n",
    "                encinput_train_data = encinput_train_dict[idx]\n",
    "                decinput_train_data = decinput_train_dict[idx]\n",
    "                dectarget_train_data = dectarget_train_dict[idx]\n",
    "                                \n",
    "                if validation_data is not None:\n",
    "                    val_idx = random.choice(list(encinput_val_dict.keys()))\n",
    "                    encinput_val_data = encinput_val_dict[val_idx]\n",
    "                    decinput_val_data = decinput_val_dict[val_idx]\n",
    "                    dectarget_val_data = dectarget_val_dict[val_idx]\n",
    "                    val_data = ([encinput_val_data, decinput_val_data, dectarget_val_data], None)\n",
    "                else:\n",
    "                    val_data = None\n",
    "                \n",
    "#                 print('example!!')\n",
    "#                 sample = decoder_predicter.predict(dectarget_train_data[:10])\n",
    "#                 print(sample[0])\n",
    "#                 print(sample.shape)\n",
    "                print('%s, %s, %s' % ( encinput_train_data.shape, decinput_train_data.shape, dectarget_train_data.shape))\n",
    "                print('%s, %s, %s' % ( encinput_val_data.shape, decinput_val_data.shape, dectarget_val_data.shape))\n",
    "                self.model.fit(x=[encinput_train_data, decinput_train_data, dectarget_train_data], \n",
    "#                                y=decoder_predicter.predict(dectarget_train_data),\n",
    "                               validation_data=val_data,\n",
    "                               batch_size=batch_size, \n",
    "                               epochs=1,\n",
    "                               verbose=verbose,\n",
    "                               validation_split=validation_split)\n",
    "#                 del encinput_train_data, decinput_train_data, dectarget_train_data\n",
    "                print('')\n",
    "            # Save model\n",
    "            self.model.save('s2s.h5')\n",
    "            \n",
    "            \n",
    "    def test(self, test_data, batch_size=64):\n",
    "        print('')\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_initial_params(*args):\n",
    "#         print(len(args))\n",
    "#         print(len(args[0]))\n",
    "        (encinput_train_dict, decinput_train_dict, dectarget_train_dict), (num_encoder_tokens, num_decoder_tokens), _ = args[0]\n",
    "        encoder_input_data, decoder_input_data, decoder_target_data = encinput_train_dict[0], decinput_train_dict[0], dectarget_train_dict[0]\n",
    "        return [encoder_input_data.shape[1], decoder_input_data.shape[1], num_encoder_tokens, num_decoder_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq2seq_try11 = Sequence2sequence(Sequence2sequence.get_initial_params(train_data), options={'loss': 'categorical_crossentropy'}, gpu=0)\n",
    "seq2seq_try11.train(train_data=train_data[0], \n",
    "              validation_data=val_data[0], \n",
    "              epochs=20, batch_size=128, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_try12 = Sequence2sequence(Sequence2sequence.get_initial_params(train_data), options={'loss': 'binary_crossentropy'}, gpu=0)\n",
    "seq2seq_try12.train(train_data=train_data[0], \n",
    "              validation_data=val_data[0], \n",
    "              epochs=20, batch_size=128, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_try13 = Sequence2sequence(Sequence2sequence.get_initial_params(train_data), options={'loss': 'mse'}, gpu=0)\n",
    "seq2seq_try13.train(train_data=train_data[0], \n",
    "              validation_data=val_data[0], \n",
    "              epochs=20, batch_size=128, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. No embedding & only use indices to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Not embedding version: size x sequence_length x 1')\n",
    "class Sequence2sequence:\n",
    "    def __init__(self, initial_params, options=dict(), gpu=0):\n",
    "        \n",
    "        max_encoder_seq_length, max_decoder_seq_length, num_encoder_tokens, num_decoder_tokens = initial_params\n",
    "        \n",
    "        self.embedding_dim = 256\n",
    "        self.latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "        \n",
    "        loss_dict = {\n",
    "            'mse': 'mse',\n",
    "            'sparse_categorical_crossentropy': 'sparse_categorical_crossentropy'\n",
    "        }\n",
    "        \n",
    "        if 'loss' in options and options['loss'] in loss_dict:\n",
    "            loss = loss_dict[options['loss']]\n",
    "        else:\n",
    "            loss = 'mse'\n",
    "        \n",
    "        with K.tf.device('/gpu:' + str(gpu)):\n",
    "            # Define an input sequence and process it.\n",
    "            encoder_inputs = Input(shape=(None, 1))\n",
    "            encoder = LSTM(self.latent_dim, return_state=True)\n",
    "            encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "            # We discard `encoder_outputs` and only keep the states.\n",
    "            encoder_states = [state_h, state_c]\n",
    "\n",
    "            # Set up the decoder, using `encoder_states` as initial state.\n",
    "            decoder_inputs = Input(shape=(None, 1))\n",
    "            # We set up our decoder to return full output sequences,\n",
    "            # and to return internal states as well. We don't use the\n",
    "            # return states in the training model, but we will use them in inference.\n",
    "            decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n",
    "            decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                                 initial_state=encoder_states)\n",
    "            decoder_dense = Dense(1, activation='relu')\n",
    "            decoder_outputs_pred = decoder_dense(decoder_outputs)\n",
    "            \n",
    "\n",
    "        # Define the model that will turn\n",
    "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs_pred)\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.encoder_states = encoder_states\n",
    "        self.decoder_lstm = decoder_lstm\n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        self.decoder_dense = decoder_dense\n",
    "\n",
    "        # Run training\n",
    "        model.compile(optimizer='rmsprop', loss=loss)\n",
    "        print(model.summary())\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def train(self, train_data, validation_data=None, epochs=50, batch_size=64, verbose=1, validation_split=0.1, *args):\n",
    "#         print(len(args))\n",
    "        encinput_train_dict, decinput_train_dict, dectarget_train_dict = train_data\n",
    "        if validation_data is not None:\n",
    "            encinput_val_dict, decinput_val_dict, dectarget_val_dict = validation_data\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print('[EPOCH %i]' % epoch)\n",
    "            for idx in encinput_train_dict.keys():\n",
    "                encinput_train_data = encinput_train_dict[idx]\n",
    "                decinput_train_data = decinput_train_dict[idx]\n",
    "                dectarget_train_data = dectarget_train_dict[idx]\n",
    "                                \n",
    "                if validation_data is not None:\n",
    "                    val_idx = random.choice(list(encinput_val_dict.keys()))\n",
    "                    encinput_val_data = encinput_val_dict[val_idx]\n",
    "                    decinput_val_data = decinput_val_dict[val_idx]\n",
    "                    dectarget_val_data = dectarget_val_dict[val_idx]\n",
    "                    val_data = ([encinput_val_data, decinput_val_data], dectarget_val_data)\n",
    "                else:\n",
    "                    val_data = None\n",
    "                \n",
    "#                 print('example!!')\n",
    "#                 sample = decoder_predicter.predict(dectarget_train_data[:10])\n",
    "#                 print(sample[0])\n",
    "#                 print(sample.shape)\n",
    "#                 print('%s, %s, %s' % ( encinput_train_data.shape, decinput_train_data.shape, dectarget_train_data.shape))\n",
    "                self.model.fit(x=[encinput_train_data, decinput_train_data], \n",
    "                               y=dectarget_train_data,\n",
    "                               validation_data=val_data,\n",
    "                               batch_size=batch_size, \n",
    "                               epochs=1,\n",
    "                               verbose=verbose,\n",
    "                               validation_split=validation_split)\n",
    "                del encinput_train_data, decinput_train_data, dectarget_train_data\n",
    "                print('')\n",
    "            # Save model\n",
    "            self.model.save('s2s.h5')\n",
    "            \n",
    "            \n",
    "    def test(self, test_data, batch_size=64):\n",
    "        print('')\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_initial_params(*args):\n",
    "#         print(len(args))\n",
    "#         print(len(args[0]))\n",
    "        (encinput_train_dict, decinput_train_dict, dectarget_train_dict), (num_encoder_tokens, num_decoder_tokens), _ = args[0]\n",
    "        encoder_input_data, decoder_input_data, decoder_target_data = encinput_train_dict[0], decinput_train_dict[0], dectarget_train_dict[0]\n",
    "        return [encoder_input_data.shape[1], decoder_input_data.shape[1], num_encoder_tokens, num_decoder_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq2seq_try21 = Sequence2sequence(Sequence2sequence.get_initial_params(train_data), options={'loss': 'mse'}, gpu=0)\n",
    "seq2seq_try21.train(train_data=train_data[0], \n",
    "              validation_data=val_data[0], \n",
    "              epochs=50, batch_size=128, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_try21 = Sequence2sequence(Sequence2sequence.get_initial_params(train_data), options={'loss': 'sparse_categorical_crossentropy'}, gpu=0)\n",
    "seq2seq_try21.train(train_data=train_data[0], \n",
    "              validation_data=val_data[0], \n",
    "              epochs=50, batch_size=128, verbose=1, validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainwave_eeg",
   "language": "python",
   "name": "brainwave_eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
