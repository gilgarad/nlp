{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Sequence to sequence copied.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gilgarad/nlp_nlu/blob/master/code_tests/LSTM_Sequence_to_sequence_copied.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YNWZC99NCVxt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I2Y0NOoRCryf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6a864cf4-c490-48d9-bcc2-a9b46a02787c"
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1EzTfQBwDMiU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Path to the data txt file on disk.\n",
        "data_path = '/content/gdrive/My Drive/ai_data/kor-eng/kor.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ATr3CF0zF262",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "31b2ccc0-6945-436c-a650-01db4913333f"
      },
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    \n",
        "print('Number of lines:', len(lines))\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of lines: 800\n",
            "Number of samples: 799\n",
            "Number of unique input tokens: 69\n",
            "Number of unique output tokens: 632\n",
            "Max sequence length for inputs: 124\n",
            "Max sequence length for outputs: 54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70_Io2dwHZi8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10761
        },
        "outputId": "2e534e85-5970-465d-f372-e64572d910f2"
      },
      "cell_type": "code",
      "source": [
        "target_characters"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\t',\n",
              " '\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'H',\n",
              " 'M',\n",
              " 'T',\n",
              " 'a',\n",
              " 'd',\n",
              " 'h',\n",
              " 'i',\n",
              " 'm',\n",
              " 'o',\n",
              " 'p',\n",
              " 'r',\n",
              " 't',\n",
              " 'y',\n",
              " '가',\n",
              " '각',\n",
              " '간',\n",
              " '갈',\n",
              " '감',\n",
              " '갑',\n",
              " '값',\n",
              " '갔',\n",
              " '강',\n",
              " '같',\n",
              " '개',\n",
              " '걀',\n",
              " '걔',\n",
              " '거',\n",
              " '걱',\n",
              " '건',\n",
              " '걸',\n",
              " '검',\n",
              " '겁',\n",
              " '것',\n",
              " '게',\n",
              " '겠',\n",
              " '겨',\n",
              " '격',\n",
              " '견',\n",
              " '결',\n",
              " '겼',\n",
              " '경',\n",
              " '계',\n",
              " '고',\n",
              " '곤',\n",
              " '곧',\n",
              " '곱',\n",
              " '곳',\n",
              " '공',\n",
              " '과',\n",
              " '관',\n",
              " '괜',\n",
              " '괴',\n",
              " '교',\n",
              " '구',\n",
              " '국',\n",
              " '군',\n",
              " '굳',\n",
              " '궁',\n",
              " '권',\n",
              " '귀',\n",
              " '그',\n",
              " '극',\n",
              " '근',\n",
              " '글',\n",
              " '금',\n",
              " '급',\n",
              " '기',\n",
              " '긴',\n",
              " '길',\n",
              " '까',\n",
              " '깎',\n",
              " '깔',\n",
              " '꺼',\n",
              " '꺾',\n",
              " '께',\n",
              " '꼈',\n",
              " '꼴',\n",
              " '꽃',\n",
              " '꽉',\n",
              " '꽤',\n",
              " '꾸',\n",
              " '꿨',\n",
              " '끄',\n",
              " '끈',\n",
              " '끊',\n",
              " '끌',\n",
              " '끓',\n",
              " '끔',\n",
              " '끝',\n",
              " '끼',\n",
              " '나',\n",
              " '난',\n",
              " '날',\n",
              " '남',\n",
              " '납',\n",
              " '났',\n",
              " '내',\n",
              " '낼',\n",
              " '냄',\n",
              " '냈',\n",
              " '냉',\n",
              " '냐',\n",
              " '냥',\n",
              " '너',\n",
              " '넌',\n",
              " '네',\n",
              " '녀',\n",
              " '녁',\n",
              " '년',\n",
              " '념',\n",
              " '녕',\n",
              " '노',\n",
              " '놀',\n",
              " '농',\n",
              " '놓',\n",
              " '놔',\n",
              " '놨',\n",
              " '누',\n",
              " '눈',\n",
              " '느',\n",
              " '는',\n",
              " '늘',\n",
              " '늙',\n",
              " '능',\n",
              " '늦',\n",
              " '니',\n",
              " '님',\n",
              " '다',\n",
              " '닥',\n",
              " '단',\n",
              " '닫',\n",
              " '달',\n",
              " '닮',\n",
              " '담',\n",
              " '답',\n",
              " '당',\n",
              " '대',\n",
              " '댔',\n",
              " '더',\n",
              " '덕',\n",
              " '던',\n",
              " '데',\n",
              " '덴',\n",
              " '도',\n",
              " '독',\n",
              " '돈',\n",
              " '돌',\n",
              " '돕',\n",
              " '동',\n",
              " '돼',\n",
              " '됐',\n",
              " '되',\n",
              " '된',\n",
              " '될',\n",
              " '됩',\n",
              " '두',\n",
              " '둔',\n",
              " '둘',\n",
              " '둬',\n",
              " '뒀',\n",
              " '뒤',\n",
              " '드',\n",
              " '든',\n",
              " '듣',\n",
              " '들',\n",
              " '듯',\n",
              " '등',\n",
              " '디',\n",
              " '딨',\n",
              " '따',\n",
              " '딸',\n",
              " '땋',\n",
              " '때',\n",
              " '떠',\n",
              " '떤',\n",
              " '떨',\n",
              " '떻',\n",
              " '뜨',\n",
              " '뜻',\n",
              " '라',\n",
              " '락',\n",
              " '란',\n",
              " '랄',\n",
              " '람',\n",
              " '랍',\n",
              " '랐',\n",
              " '랑',\n",
              " '래',\n",
              " '랜',\n",
              " '랩',\n",
              " '러',\n",
              " '럭',\n",
              " '런',\n",
              " '럴',\n",
              " '럼',\n",
              " '럽',\n",
              " '렀',\n",
              " '렇',\n",
              " '레',\n",
              " '렌',\n",
              " '려',\n",
              " '렵',\n",
              " '렸',\n",
              " '례',\n",
              " '로',\n",
              " '록',\n",
              " '론',\n",
              " '롭',\n",
              " '료',\n",
              " '루',\n",
              " '류',\n",
              " '륙',\n",
              " '르',\n",
              " '른',\n",
              " '를',\n",
              " '름',\n",
              " '릅',\n",
              " '리',\n",
              " '린',\n",
              " '릴',\n",
              " '림',\n",
              " '마',\n",
              " '막',\n",
              " '만',\n",
              " '많',\n",
              " '말',\n",
              " '맛',\n",
              " '망',\n",
              " '맞',\n",
              " '맡',\n",
              " '매',\n",
              " '머',\n",
              " '먹',\n",
              " '멍',\n",
              " '메',\n",
              " '멕',\n",
              " '면',\n",
              " '명',\n",
              " '몇',\n",
              " '모',\n",
              " '목',\n",
              " '몰',\n",
              " '못',\n",
              " '무',\n",
              " '묶',\n",
              " '문',\n",
              " '묻',\n",
              " '물',\n",
              " '뭐',\n",
              " '뭔',\n",
              " '뭘',\n",
              " '미',\n",
              " '믿',\n",
              " '밀',\n",
              " '바',\n",
              " '박',\n",
              " '밖',\n",
              " '반',\n",
              " '받',\n",
              " '발',\n",
              " '밟',\n",
              " '밤',\n",
              " '방',\n",
              " '배',\n",
              " '백',\n",
              " '뱀',\n",
              " '버',\n",
              " '번',\n",
              " '벌',\n",
              " '법',\n",
              " '벗',\n",
              " '베',\n",
              " '벼',\n",
              " '벽',\n",
              " '변',\n",
              " '별',\n",
              " '병',\n",
              " '보',\n",
              " '복',\n",
              " '본',\n",
              " '볼',\n",
              " '봄',\n",
              " '봅',\n",
              " '봉',\n",
              " '봐',\n",
              " '봤',\n",
              " '부',\n",
              " '북',\n",
              " '분',\n",
              " '불',\n",
              " '붉',\n",
              " '붐',\n",
              " '비',\n",
              " '빈',\n",
              " '빌',\n",
              " '빛',\n",
              " '빠',\n",
              " '빨',\n",
              " '빴',\n",
              " '빵',\n",
              " '빼',\n",
              " '뻔',\n",
              " '뿐',\n",
              " '사',\n",
              " '산',\n",
              " '살',\n",
              " '삼',\n",
              " '샀',\n",
              " '상',\n",
              " '새',\n",
              " '색',\n",
              " '생',\n",
              " '서',\n",
              " '석',\n",
              " '선',\n",
              " '설',\n",
              " '성',\n",
              " '세',\n",
              " '셔',\n",
              " '셨',\n",
              " '소',\n",
              " '속',\n",
              " '손',\n",
              " '솔',\n",
              " '송',\n",
              " '쇠',\n",
              " '수',\n",
              " '숙',\n",
              " '순',\n",
              " '술',\n",
              " '숨',\n",
              " '쉽',\n",
              " '슈',\n",
              " '스',\n",
              " '슨',\n",
              " '슬',\n",
              " '습',\n",
              " '시',\n",
              " '식',\n",
              " '신',\n",
              " '실',\n",
              " '싫',\n",
              " '심',\n",
              " '십',\n",
              " '싶',\n",
              " '싸',\n",
              " '쌉',\n",
              " '써',\n",
              " '썼',\n",
              " '썽',\n",
              " '쓰',\n",
              " '쓸',\n",
              " '씀',\n",
              " '씨',\n",
              " '씻',\n",
              " '아',\n",
              " '안',\n",
              " '앉',\n",
              " '않',\n",
              " '알',\n",
              " '압',\n",
              " '았',\n",
              " '앙',\n",
              " '앞',\n",
              " '애',\n",
              " '야',\n",
              " '약',\n",
              " '양',\n",
              " '얘',\n",
              " '어',\n",
              " '억',\n",
              " '언',\n",
              " '얼',\n",
              " '엄',\n",
              " '업',\n",
              " '없',\n",
              " '엇',\n",
              " '었',\n",
              " '에',\n",
              " '엔',\n",
              " '여',\n",
              " '역',\n",
              " '연',\n",
              " '열',\n",
              " '염',\n",
              " '였',\n",
              " '영',\n",
              " '옆',\n",
              " '예',\n",
              " '옛',\n",
              " '오',\n",
              " '옥',\n",
              " '온',\n",
              " '올',\n",
              " '옳',\n",
              " '옷',\n",
              " '와',\n",
              " '완',\n",
              " '왔',\n",
              " '왜',\n",
              " '외',\n",
              " '요',\n",
              " '용',\n",
              " '우',\n",
              " '운',\n",
              " '울',\n",
              " '움',\n",
              " '웃',\n",
              " '워',\n",
              " '원',\n",
              " '월',\n",
              " '웠',\n",
              " '웨',\n",
              " '위',\n",
              " '윗',\n",
              " '유',\n",
              " '육',\n",
              " '으',\n",
              " '은',\n",
              " '을',\n",
              " '음',\n",
              " '읍',\n",
              " '의',\n",
              " '이',\n",
              " '인',\n",
              " '일',\n",
              " '읽',\n",
              " '잃',\n",
              " '임',\n",
              " '입',\n",
              " '있',\n",
              " '잊',\n",
              " '자',\n",
              " '작',\n",
              " '잔',\n",
              " '잖',\n",
              " '잘',\n",
              " '잠',\n",
              " '잡',\n",
              " '잤',\n",
              " '장',\n",
              " '재',\n",
              " '쟁',\n",
              " '저',\n",
              " '적',\n",
              " '전',\n",
              " '절',\n",
              " '점',\n",
              " '접',\n",
              " '정',\n",
              " '제',\n",
              " '져',\n",
              " '졌',\n",
              " '조',\n",
              " '족',\n",
              " '존',\n",
              " '졸',\n",
              " '좀',\n",
              " '종',\n",
              " '좋',\n",
              " '죄',\n",
              " '죠',\n",
              " '주',\n",
              " '죽',\n",
              " '준',\n",
              " '줄',\n",
              " '중',\n",
              " '줘',\n",
              " '줬',\n",
              " '즐',\n",
              " '즘',\n",
              " '증',\n",
              " '지',\n",
              " '직',\n",
              " '진',\n",
              " '질',\n",
              " '집',\n",
              " '짓',\n",
              " '짜',\n",
              " '짝',\n",
              " '쪼',\n",
              " '쪽',\n",
              " '쫓',\n",
              " '찌',\n",
              " '찍',\n",
              " '찢',\n",
              " '차',\n",
              " '착',\n",
              " '찮',\n",
              " '참',\n",
              " '찼',\n",
              " '창',\n",
              " '찾',\n",
              " '채',\n",
              " '책',\n",
              " '챘',\n",
              " '처',\n",
              " '척',\n",
              " '천',\n",
              " '철',\n",
              " '청',\n",
              " '체',\n",
              " '쳐',\n",
              " '쳤',\n",
              " '초',\n",
              " '최',\n",
              " '추',\n",
              " '축',\n",
              " '출',\n",
              " '충',\n",
              " '취',\n",
              " '츠',\n",
              " '치',\n",
              " '칙',\n",
              " '친',\n",
              " '침',\n",
              " '카',\n",
              " '캐',\n",
              " '커',\n",
              " '컴',\n",
              " '켓',\n",
              " '켜',\n",
              " '코',\n",
              " '쾅',\n",
              " '퀴',\n",
              " '크',\n",
              " '큰',\n",
              " '큼',\n",
              " '키',\n",
              " '타',\n",
              " '탁',\n",
              " '탈',\n",
              " '탐',\n",
              " '탓',\n",
              " '탔',\n",
              " '태',\n",
              " '터',\n",
              " '턱',\n",
              " '턴',\n",
              " '테',\n",
              " '텐',\n",
              " '토',\n",
              " '톰',\n",
              " '통',\n",
              " '퇴',\n",
              " '투',\n",
              " '트',\n",
              " '틀',\n",
              " '티',\n",
              " '파',\n",
              " '판',\n",
              " '팔',\n",
              " '패',\n",
              " '퍼',\n",
              " '페',\n",
              " '편',\n",
              " '평',\n",
              " '포',\n",
              " '폭',\n",
              " '폰',\n",
              " '푹',\n",
              " '풀',\n",
              " '퓨',\n",
              " '프',\n",
              " '피',\n",
              " '필',\n",
              " '하',\n",
              " '학',\n",
              " '한',\n",
              " '할',\n",
              " '함',\n",
              " '합',\n",
              " '항',\n",
              " '해',\n",
              " '했',\n",
              " '행',\n",
              " '향',\n",
              " '햿',\n",
              " '허',\n",
              " '헌',\n",
              " '험',\n",
              " '혀',\n",
              " '현',\n",
              " '혈',\n",
              " '혐',\n",
              " '혔',\n",
              " '형',\n",
              " '혜',\n",
              " '호',\n",
              " '혼',\n",
              " '홉',\n",
              " '화',\n",
              " '확',\n",
              " '환',\n",
              " '활',\n",
              " '회',\n",
              " '획',\n",
              " '효',\n",
              " '후',\n",
              " '훔',\n",
              " '휘',\n",
              " '휴',\n",
              " '흔',\n",
              " '흙',\n",
              " '흡',\n",
              " '흥',\n",
              " '희',\n",
              " '히',\n",
              " '힘']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "TsLbs-GRCgt_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8588
        },
        "outputId": "f4ee6801-bc30-4cb4-bde7-7a03b160dfdf"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 50  # Number of epochs to train for.\n",
        "latent_dim = 64  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "# Save model\n",
        "model.save('s2s.h5')\n",
        "\n",
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state\n",
        "# and a \"start of sequence\" token as target.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with the current target token and current states\n",
        "\n",
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "for seq_index in range(100):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 639 samples, validate on 160 samples\n",
            "Epoch 1/50\n",
            "639/639 [==============================] - 4s 7ms/step - loss: 1.5862 - val_loss: 2.4038\n",
            "Epoch 2/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.2724 - val_loss: 2.0994\n",
            "Epoch 3/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.1537 - val_loss: 2.0450\n",
            "Epoch 4/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.1239 - val_loss: 2.0514\n",
            "Epoch 5/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.1117 - val_loss: 2.0426\n",
            "Epoch 6/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.1035 - val_loss: 2.0378\n",
            "Epoch 7/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0962 - val_loss: 2.0457\n",
            "Epoch 8/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0915 - val_loss: 2.0500\n",
            "Epoch 9/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0869 - val_loss: 2.0390\n",
            "Epoch 10/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0834 - val_loss: 2.0562\n",
            "Epoch 11/50\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 1.0777 - val_loss: 2.0500\n",
            "Epoch 12/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0749 - val_loss: 2.0411\n",
            "Epoch 13/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0701 - val_loss: 2.0450\n",
            "Epoch 14/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0666 - val_loss: 2.0800\n",
            "Epoch 15/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0634 - val_loss: 2.0534\n",
            "Epoch 16/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0579 - val_loss: 2.0493\n",
            "Epoch 17/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0555 - val_loss: 2.0781\n",
            "Epoch 18/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0505 - val_loss: 2.0848\n",
            "Epoch 19/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0469 - val_loss: 2.0253\n",
            "Epoch 20/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0437 - val_loss: 2.0512\n",
            "Epoch 21/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0389 - val_loss: 2.0609\n",
            "Epoch 22/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0345 - val_loss: 2.0289\n",
            "Epoch 23/50\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 1.0303 - val_loss: 2.0898\n",
            "Epoch 24/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0236 - val_loss: 2.1057\n",
            "Epoch 25/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0194 - val_loss: 2.0465\n",
            "Epoch 26/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0155 - val_loss: 2.0492\n",
            "Epoch 27/50\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 1.0090 - val_loss: 2.0291\n",
            "Epoch 28/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 1.0046 - val_loss: 1.9833\n",
            "Epoch 29/50\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 0.9996 - val_loss: 1.9943\n",
            "Epoch 30/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9946 - val_loss: 1.9985\n",
            "Epoch 31/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9876 - val_loss: 2.0163\n",
            "Epoch 32/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9823 - val_loss: 1.9538\n",
            "Epoch 33/50\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 0.9758 - val_loss: 2.0069\n",
            "Epoch 34/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9708 - val_loss: 1.9634\n",
            "Epoch 35/50\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 0.9629 - val_loss: 2.0024\n",
            "Epoch 36/50\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 0.9585 - val_loss: 1.9688\n",
            "Epoch 37/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9533 - val_loss: 1.9404\n",
            "Epoch 38/50\n",
            "639/639 [==============================] - 3s 5ms/step - loss: 0.9461 - val_loss: 1.9995\n",
            "Epoch 39/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9429 - val_loss: 1.9416\n",
            "Epoch 40/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9349 - val_loss: 1.9337\n",
            "Epoch 41/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9277 - val_loss: 1.8890\n",
            "Epoch 42/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9307 - val_loss: 1.9044\n",
            "Epoch 43/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9173 - val_loss: 1.9026\n",
            "Epoch 44/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9200 - val_loss: 1.8842\n",
            "Epoch 45/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9099 - val_loss: 1.9013\n",
            "Epoch 46/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.9070 - val_loss: 1.8871\n",
            "Epoch 47/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.8959 - val_loss: 1.9027\n",
            "Epoch 48/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.8899 - val_loss: 1.9066\n",
            "Epoch 49/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.8857 - val_loss: 1.8591\n",
            "Epoch 50/50\n",
            "639/639 [==============================] - 3s 4ms/step - loss: 0.8782 - val_loss: 1.8453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_3/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm sad.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Me, too.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Perfect!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Shut up!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Welcome.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Welcome.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Cheer up!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Get lost.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm ugly.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: It hurts.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Let's go!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Don't lie.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Don't lie.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm sorry.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm sorry.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm sorry.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm sorry.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Of course.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Seriously?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Take care.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Be careful.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: He is nice.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Hold still.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Hold still.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: How lovely!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I felt bad.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Is that OK?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Love hurts.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Boys do cry.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I don't lie.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I don't lie.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I don't lie.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm nervous.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm nervous.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm shocked.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: It's a pity.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: What's that?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: You're mine.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: You're mine.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Blood is red.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Come quickly!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Come quickly!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Don't eat it.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I don't know.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I hate liars.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I need money.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Is that okay?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Is this mine?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Is this wine?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: It's suicide.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Just keep it.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: We don't lie.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: We don't lie.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: We're inside.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: We're inside.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: What is that?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Can I ask why?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Grab the rope.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I am homesick.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I can't sleep.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I feel guilty.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I hate myself.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I smell blood.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I use Firefox.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I want to die.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I want to die.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'll kill him.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm depressed.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Is that blood?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: My head hurts.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: We want peace.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Autumn is here.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Autumn is here.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Can I help you?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Do you hear me?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: He was hard up.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I don't buy it.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I like reading.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I love lasagna.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I love my home.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I study Korean.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm very sorry.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: I'm very sorry.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Keep Tom there.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Only God knows.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Read this book.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Read this book.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Sorry I'm late.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: That's suicide.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Boil some water.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Can you help me?\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Congratulations!\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Do you like rap?\n",
            "Decoded sentence: .\n",
            "\n",
            "-\n",
            "Input sentence: Do you like rap?\n",
            "Decoded sentence: .\n",
            "\n",
            "-\n",
            "Input sentence: Don't lie to me.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n",
            "-\n",
            "Input sentence: Don't lie to me.\n",
            "Decoded sentence: 톰은       어.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XBSVrlY0EgOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}